\documentclass[10pt,a4paper]{report}
\usepackage[hmargin=2cm,vmargin=3.5cm,bmargin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[pdftex]{hyperref}
\usepackage{csvsimple}
\usepackage{color}
\usepackage{mathtools}
\usepackage{float}

% Pseudocode
\usepackage{algpseudocode}


% Headers and Footers
\usepackage{fancyhdr}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keywordstyle=\color{blue},       % keyword style
%  language=Octave,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\lstdefinestyle{base}{
  language=C++,
  emptylines=1,
  breaklines=true,
  basicstyle=\ttfamily\color{black},
  moredelim=**[is][\color{blue}]{@}{@},
  basicstyle=\footnotesize
}

% Headers and Footers
\pagestyle{fancy} % fancy style
\lhead{\rightmark} % left header
\rhead{\leftmark} % right header
\lfoot{} % left footer
\cfoot{\textbf{\thepage}} % central footer
\rfoot{} % right footer
% Headers and Footers

\makeindex

% define the title
\author{Lucas Henrique Morais\\ Lu√≠s Felipe Mattos}
\title{\textbf{Parboil:} an optimization effort}
\date{October 11, 2013}
\begin{document}

% generates the title
\maketitle

% insert the table of contents
	\tableofcontents

\chapter{General Methods}

In this study we identified two optimization techniques that are very broad in scope and that would be recurrently employed in our solutions for each benchmark. These are as follows:
\begin{itemize}
\item Lock-free vector accumulation
\item Prescient memory access
\end{itemize}

Which we describe in detail in the following sections.
\section{Lock-free vector accumulation}
A very general optimization technique that was consistently employed in the studied set of benchmarks was lock-free vector accumulation. Let us introduce the topic with the following example:
Usually, one could trivially avoid declaring a critical section on a piece of code like this:
	     
\begin{lstlisting}[caption=0-dimensional accumulation,style=base]
int foo( ... ){
	#pragma omp parallel
	for(i = 0; i < size; i++){
		//many lines of code
		#pragma omp critical section
		bin += aux;
	}
}
\end{lstlisting}	     
	     
Usually, one could trivially avoid declaring the critical section above by:
\begin{enumerate}
\item Declaring a private variable for each thread.
\item Requiring each thread to accumulate on its own version of the variable.
\item Consolidating the partial sums on the original variable.
\end{enumerate}
	  
The present report then suggests a generalization of that approach for not only lock-free accumulation on unidimensional arrays, but for n-dimensional vectors, with negligible resulting overhead.
\newpage
\begin{lstlisting}[caption=4-dimensional accumulation,style=base]
int foo( ... ){
	#pragma omp parallel
	for(i = 0; i < size; i++){
		//many lines of code
		#pragma omp critical section
		bin[p1][p2][p3][p4] += aux;
	}
}
\end{lstlisting}	     
	     
The overall strategy is comprised of three main elements, as follows:
\begin{enumerate}
\item Creation of an \((n+1)\)-dimensional auxiliar vector.
\item Private accumulation of each thread's work on its respective n-dimensional data strip.
\item Parallel consolidation of the results on the target (original) vector.
\end{enumerate}
(* Here we assume a $n$-dimensional target vector.)
After the first one, each step is triggered by the end of the preceeding one. Each is now described in detail:	     
\begin{description}
\item[Replication]
At the end of this step, we want each thread to have its own copy of the original structure. This can be easily accomplished by allocating a vector wrapping k copies of the original D.S. We then have a $(n+1)$ dimensional vector on memory.	     	   
\item[Private accumulation]
Each thread then works on its own data share and privately accumulate on its own data strip. Generally, a $j$-numbered thread shall access memory locations of the format \[V_{aux}[j][*][*]\ldots[*],\]where $*$ can assume any available value.
\item[Consolidation]
By this time, all working data has been consumed, and we are left with the task of summing up all the partial results on the original structure, which we can fortunately do in parallel for any n greater than zero, as follows:
\begin{description}	     	   
\item[Partitioning]
Each thread is associated with a subset of the target structure memory positions.
This can be automatically achieved with OpenMP \emph{parallel for} pragma.
\item[Actual consolidation]
Then, each thread computes the following expression:	     	   
\[V_{final}\{\pi_j\}=V_{aux}[0]\{\pi_j\}+V_{aux}[1]\{\pi_j\}+\ldots+V_{aux}[n-1]\{\pi_j\}\]
	     	   where \(n \) is the total number of working threads and \(\pi_j\) indicates the partition of the memory positions that j-numbered thread is responsible for. This expression is iteratively computed. As partition sets are always disjoint, it follows that this step can be trivially parallelized with, for example, a \emph{parallel for} pragma as suggested above.
\newline 
\newline
For our example, we would now have:

\begin{lstlisting}[caption=4-dimensional accumulation,style=base]
int foo( ... ){
	#pragma omp parallel
	for(i = 0; i < size; i++){
		//many lines of code
		big_bin[thread_num][p1][p2][p3][p4] += aux;
	}
	
	#pragma omp parallel
	for (i = 0; i < num_threads; i++)
		for (j = 0; j < j_max; j++)
			for (k = 0; k < k_max; k++)
				for (l = 0; l < l_max; l++)
					for	(m = 0; m < m_max; m++){
						bin = big_bin[i][j][k][l][m];
					}
}
\end{lstlisting}	 
	     	   
\end{description}
\end{description}
\section{Prescient memory access}
One of the greatest optimizations we applied in several algorithms was the better use of the cache memory and the main memory. Some data intensive reading algorithms can benefit from the cache memory by accessing sequential adresses from the main memory, with lesser cache misses, the algorithm has no need to search the data in the main memory, which is much slower than low level memories. \\

By changing the way the memory is accessed, the performance was improved in the sequential version of some algorithms, improving even the performance over some simple parallelized versions.\\

Several algorithms in this benchmark set have nested loops and access some structure, like a array or a matrix, inside the inner loop. Also, some algorithms have inefficient index choice for the inner loop, turning the memory access into a slow and repetitious task. Some changes that improved the performance include the remapping of the structure or/and the better choice of the order for the indexes in the nested loops. \\


\chapter{Optimizations}

\newpage
\section{sgemm}
\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential source code for sgemm,style=base]

	for (int mm = 0; mm < m; ++mm) {
		for (int nn = 0; nn < n; ++nn) {
			float c = 0.0f;
			for (int i = 0; i < k; ++i) {
				float a = A[mm + i * lda]; 
				float b = B[nn + i * ldb];		
				c += a * b;
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel source code for sgemm,style=base]

	@#pragma omp parallel for collapse (2)@
	for (int mm = 0; mm < m; ++mm) {
		for (int nn = 0; nn < n; ++nn) {
			float c = 0.0f;
			for (int i = 0; i < k; ++i) {
				float a = A[mm + i * lda]; 
				float b = B[nn + i * ldb];
				c += a * b;
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}
\end{minipage}
\end{center}

For the matrices multiplication algorithm, it is used an array-represented structure, so, we improved the memory access by transposing the matrices. This way, the index 'i' in the inner loop, is only added instead of multiplied to calculate the correct mapping of the element in the matrix to the array-represented matrix. We needed then to aloccate auxiliary arrays to represent the transposed matrices and then transpose while making the copy from one array to another. \\

To avoid the increase of the execution time of the algorithm, we parallelized this transposition. This can be done because it has no loop-carried dependencies and the data is distributed almost equally for all threads without concurrency, in other words, it is high parallelizable. \\

We created the variables aux1 and aux2 because in the inner loop, these values are constant. The original versions, the sequential and the stock parallel, calculate these constant every iteration of the inner loop, wasting some time in unnecessary multiplications. \\

The access to the array is almost sequential after this changes. Using better the memory, caused some great improvement in execution time of this algoritm. By applying these techniques we could ran our parallel version around 10.7 times faster than sequential version and 3.9 times faster than the stock parallel version.\\

\begin{lstlisting}[caption=Array-represented matrices transposition,style=base]
	tb = (float *) malloc(ldb * n * sizeof(float));
	@# pragma omp parallel for collapse(2)@
	for (i = 0; i < ldb; i++)
		for (j = 0; j < n; j++)
			tb[j * n + i] = B[i * n + j];

	ta = (float *) malloc(lda * m * sizeof(float));
	@# pragma omp parallel for collapse(2)@
	for (i = 0; i < lda; i++)
		for (j = 0; j < m; j++)
			ta[j * m + i] = A[i * m + j];
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel source code for sgemm,style=base]
	@# pragma omp parallel for private(nn, c, i, a, b, mm) collapse(2) schedule (static)@
	for (mm = 0; mm < m; ++mm) {
		for (nn = 0; nn < n; ++nn) {
			c = 0.0f;
			aux1 = mm * lda;
			aux2 = nn * ldb;
			for (i = 0; i < k; ++i) {
				c += ta[i + aux1] * tb[i + aux2];
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}

\newpage
\section{stencil}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential source code for stencil,style=base]

	int i, j, k;
	for(i=1;i<nx-1;i++) {
		for(j=1;j<ny-1;j++) {
			for(k=1;k<nz-1;k++) {
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel source code for stencil,style=base]

	int i;
	@#pragma omp parallel for@
	for(i=1;i<nx-1;i++) {
		int j, k;
		for(j=1;j<ny-1;j++) {
			for(k=1;k<nz-1;k++) {
				//#pragma omp critical
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}
\end{minipage}\\
\begin{lstlisting}[caption=Converts from 3D indices to array-represented cube,style=base]	
	#define Index3D(_nx,_ny,_i,_j,_k) ((_i)+_nx*((_j)+_ny*(_k)))
\end{lstlisting}
\end{center}

The source codes above, specially the listing 2.7, show that the access to the cube elements are made by using the inner loop counter to multiply some constant and then map the cube to an array. The sequential access to memory is lost and the performance is impaired by sucessive cache misses. \\

Basically, we exchanged the index from the inner loop with the index of the outer loop. Then we simply parallelized the outer loop, which made our version run around 25.4 times faster than sequential version and around 15.2 times faster than the stock parallel version.\\

\begin{lstlisting}[caption=Our parallel source code for stencil,style=base]
	
	int i, j, k;	
	@#pragma omp parallel for private(j, i)@
	for(k=1;k<nz-1;k++) {
		for(j=1;j<ny-1;j++) {
			for(i=1;i<nx-1;i++) {
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}

\newpage
\section{mri-gridding}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Auxiliary function source code for sequential mri-gridding,style=base]

		unsigned int k;
		for(k=0; k<size; ++k){
			// compute value to evaluate kernel at
			// v in the range 0:(_width/2)^2
			v = (((float)k)/((float)size))*cutoff2;

			// compute kernel value and store
			(*LUT)[k] = kernel_value_CPU(beta*sqrt(1.0-(v/cutoff2)));
		}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Auxiliary function source code for parallel mri-gridding,style=base]

		unsigned int k;
		@#pragma omp parallel for private(v)@
		for(k=0; k<size; ++k){
			// compute value to evaluate kernel at
			// v in the range 0:(_width/2)^2
			v = (((float)k)/((float)size))*cutoff2;

			// compute kernel value and store
			(*LUT)[k] = kernel_value_CPU(beta*sqrt(1.0-(v/cutoff2)));
		}
\end{lstlisting}
\end{minipage}
\end{center}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential main loop for mri-gridding,style=base]

	int i;
	for (i=0; i < n; i++){
		ReconstructionSample pt = sample[i];

		float kx = pt.kX;
		float ky = pt.kY;
		float kz = pt.kZ;
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel main loop for mri-gridding,style=base]

	int i;

	@#pragma omp parallel for private(NxL, NxH, NyL, NyH, NzL, NzH, dz2, nz, dx2, nx, dy2, ny, idxZ, idxY, dy2dz2, idx0, v, idx, w)@
	for (i=0; i < n; i++){
		ReconstructionSample pt = sample[i];

		float kx = pt.kX;
		float ky = pt.kY;
		float kz = pt.kZ;
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential data processing code for mri-gridding,style=base]
		/* grid data */
		gridData[idx].real += (w*pt.real);
		gridData[idx].imag += (w*pt.imag);

		/* estimate sample density */
		sampleDensity[idx] += 1.0;
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel data processing code for mri-gridding,style=base]
		/* grid data */
		@#pragma omp critical (c1)@
		gridData[idx].real += (w*pt.real);
		@#pragma omp critical (c2)@
		gridData[idx].imag += (w*pt.imag);

		/* estimate sample density */
		@#pragma omp critical (c3)@
		sampleDensity[idx] += 1.0;
\end{lstlisting}
\end{minipage}
\end{center}

For this algorithm we basically allocated the lock-free structure and the accumulations are made privately and without concurrency for each thread. When all threads have finished the work, the final result is obtained by accumulating the partial results from each thread. Luckly, there is no loop-carried dependency, and the partial and the final accumulations are high parallelizable.\\

Our parallel version, with the lock-free vector accumulation, have a execution time around 2.5 times faster than the sequential and around 41.6 times faster than the stock parallel version. This happens because of the critical section pragmas, as we can see on listing 2.14. Each thread must execute this regions with a lock to obtain the correct result. This way, all other threads wait to get the lock and enter the critical region. By using this lock with 8 threads, there is a lot of concurrency and racing conditions, making the stock parallel the slowest version we tested, slower even the sequential version.\\

\newpage
\begin{center}
\begin{lstlisting}[caption=Our parallel lock-free structure allocation for mri-gridding,style=base]

	@#pragma omp parallel private(NxL, NxH, NyL, NyH, NzL, NzH, dz2, nz, dx2,         \
		nx, dy2, ny, idxZ, idxY, dy2dz2, idx0, v, idx, w, t_id)@
	{
		@#pragma omp single@
		{
			if (big_table == NULL)
			{
				//			  printf("Alocou!\n");
				size = params.gridSize[0]*params.gridSize[1]*params.gridSize[2];
				numthreads = omp_get_max_threads();	  
				big_table = (cmplx **) malloc(numthreads * sizeof(cmplx *));
				big_table2 = (float **) malloc(numthreads * sizeof(float *));
			}
		}

		@#pragma omp for@
		for (i = 0; i < numthreads; i++)
		{
			big_table[i] = (cmplx *) malloc(size * sizeof(cmplx));
			big_table2[i] = (float *) malloc(size * sizeof(float));
		}
		@#pragma omp for collapse(2)@
		for (i = 0; i < numthreads; i++)
			for (j = 0; j < size; j++)
			{
				big_table[i][j].real = 0;
				big_table[i][j].imag = 0;
				big_table2[i][j] = 0;
			}

		t_id = omp_get_thread_num();

		@#pragma omp for@
		for (i = 0; i < n; i++)
		{
			ReconstructionSample pt = sample[i];

			float kx = pt.kX;
			float ky = pt.kY;
			float kz = pt.kZ;
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel partial data processing code for mri-gridding,style=base]
		/* grid data */
		big_table[t_id][idx].real += (w*pt.real);
		big_table[t_id][idx].imag += (w*pt.imag);

		/* estimate sample density */
		big_table2[t_id][idx] += 1.0;		
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel final data processing code for mri-gridding,style=base]
	#pragma omp for private(j)
	for (i = 0; i < size; i++)
		for (j = 0; j < numthreads; j++)
		{
			gridData[i].real += big_table[j][i].real;
			gridData[i].imag += big_table[j][i].imag;
			sampleDensity[i] += big_table2[j][i];
		}
\end{lstlisting}


\end{center}
\section{cutcp}
Basically, we implemented an auxiliary data structure that allowed for a lock-free version of the core routine of the benchmark, which eliminated the critical section pointed out in line 12. By doing that, we accomplished a speed up of around 25\% in relation to the stock parallelization \((\)omp\_base\()\).

\begin{lstlisting}[caption=cutcp serial version source,style=base]
for (gindex = 0;  gindex < ncell;  gindex++) {
    for (n = first[gindex];  n != -1;  n = next[n]) {
    //ommited code
    	for (k = ka;  k <= kb;  k++, dz += gridspacing) {
    	//ommited code
    		for (j = ja;  j <= jb;  j++, dy += gridspacing) {
    		//ommited code
    			for (i = ia;  i <= ib;  i++, pg++, dx += gridspacing) {
    					r2 = dx*dx + dydz2;
   						 s = (1.f - r2 * inv_a2);
   						 e = q * (1/sqrtf(r2)) * s * s;
							*pg += e;
          }
        }
      }
    }
  }
\end{lstlisting}


\begin{lstlisting}[caption=Original parallel cutcp source code,style=base]
#pragma omp parallel for private ( ... )
for (gindex = 0;  gindex < ncell;  gindex++) {
    for (n = first[gindex];  n != -1;  n = next[n]) {
    //ommited code
    	for (k = ka;  k <= kb;  k++, dz += gridspacing) {
    	//ommited code
    		for (j = ja;  j <= jb;  j++, dy += gridspacing) {
    		//ommited code
    			for (i = ia;  i <= ib;  i++, pg++, dx += gridspacing) {
					s = (1.f - r2 * inv_a2);
					e = q * (1/sqrtf(r2)) * s * s;
					@#pragma omp atomic@
					*pg += e;
          }
        }
      }
    }
  }
\end{lstlisting}


\begin{lstlisting}[caption=Our parallel source code for cutcp,,style=base]
  nthreads = omp_get_max_threads();  
  big_table = (float **) malloc(nthreads * sizeof(float *));
  for (i = 0; i < nthreads; i++)
  	big_table[i] = calloc(lattice->size, sizeof(float));
  	
 @#pragma omp parallel private ( ... ) @{
	g = big_table[omp_get_thread_num()];
		#pragma omp for
			for (gindex = 0;  gindex < ncell;  gindex++) {
				for (n = first[gindex];  n != -1;  n = next[n]) {
				//ommited code
					for (k = ka;  k <= kb;  k++, dz += gridspacing) {
					//ommited code
						for (j = ja;  j <= jb;  j++, dy += gridspacing) {
						//ommited code
							for (i = ia, m = index;  i <= ib;  i++, m++, dx += gridspacing) {
								pg[m] += (q/sqrtf(r2)) * (1.f - r2 * inv_a2) * (1.f - r2 * inv_a2);
							}
						}
					}
				}
			}	  
	  @#pragma omp for private(i)@
	  for (m = 0; m < nthreads; m++)
		  for (i = 0; i < lattice->size; i++)
		  	lattice->lattice[i] += big_table[m][i];
} 	
\end{lstlisting}

\section{tpacf}
Here we carried out a strategy very akin to that indicated in the last section,which mainly consisted of rethinking the vector accumulation in such a way that critical sections could be circumvented.

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=tpacf serial version source,style=base]
int doCompute( ... ){  
	for (i = 0; i < ((doSelf) ? n1-1 : n1); i++){
		for (j = ((doSelf) ? i+1 : 0); j < n2; j++){	  
			while (max > min+1){
				k = (min + max) / 2;
				if (dot >= binb[k]) 
					max = k;
				else 
					min = k;
			};
			if (dot >= binb[min])
				data_bins[min] += 1;
			else if (dot < binb[max]) 
				data_bins[max+1] += 1;
			else 
				data_bins[max] += 1;
		}
	}
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=tpacf stock parallel solution,style=base]
int doCompute( ... ){  
	for (i = 0; i < ((doSelf) ? n1-1 : n1); i++){
		@#pragma omp parallel for @
		for (j = ((doSelf) ? i+1 : 0); j < n2; j++){	  
			while (max > min+1){
				k = (min + max) / 2;
				if (dot >= binb[k]) 
					max = k;
				else 
					min = k;
			};
			@#pragma omp critical@
			if (dot >= binb[min])
				data_bins[min] += 1;
			else if (dot < binb[max]) 
				data_bins[max+1] += 1;
			else 
				data_bins[max] += 1;
		}
	}
}
\end{lstlisting}
\end{minipage}
\end{center}



\begin{lstlisting}[caption=Our tpacf parallel version,style=base]
int doCompute( ... ){
	@#pragma omp parallel@{
		@#pragma omp single@{
			if (doSelf){
				n2 = n1;
				data2 = data1;
			}
			size = ((doSelf) ? n1-1 : n1);
			nthreads = omp_get_num_threads();
			big_table = (int **) malloc(nthreads * sizeof(int *));
		}		
		@#pragma omp for@
		for (i = 0; i < nthreads; i++)
			big_table[i] = (int *) malloc((nbins + 1) * sizeof(int));
		@#pragma omp for private(j)@
		for (i = 0; i < ((doSelf) ? n1-1 : n1); i++){
			for (j = ((doSelf) ? i+1 : 0); j < n2; j++){	  
				while (max > min+1){
					k = (min + max) / 2;
					if (dot >= binb[k]) max = k;
					else min = k;
				};
				if (dot >= binb[min]) k = min;					
				else if (dot < binb[max]) k = max+1;					
				else k = max;					
				big_table[omp_get_thread_num()][k]++;
			}
			@#pragma omp for private(j)@
			for (i = 0; i < nbins + 1; i++)
				for (j = 0; j < nthreads; j++)
					data_bins[i] += big_table[j][i];			
			@#pragma omp for@
				for (i = 0; i < nthreads; i++)
					free(big_table[i]);
		}
	}
    free(big_table);
}
\end{lstlisting}



\end{document}
