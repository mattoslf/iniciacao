\documentclass[10pt,a4paper]{report}
\usepackage[hmargin=2cm,vmargin=3.5cm,bmargin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[pdftex]{hyperref}
\usepackage{csvsimple}
\usepackage{color}
\usepackage{mathtools}
\usepackage{float}

% Pseudocode
\usepackage{algpseudocode}


% Headers and Footers
\usepackage{fancyhdr}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keywordstyle=\color{blue},       % keyword style
%  language=Octave,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\lstdefinestyle{base}{
  language=C++,
  emptylines=1,
  breaklines=true,
  basicstyle=\ttfamily\color{black},
  moredelim=**[is][\color{blue}]{@}{@},
  basicstyle=\footnotesize
}

% Headers and Footers
\pagestyle{fancy} % fancy style
\lhead{\rightmark} % left header
\rhead{\leftmark} % right header
\lfoot{} % left footer
\cfoot{\textbf{\thepage}} % central footer
\rfoot{} % right footer
% Headers and Footers

\makeindex

% define the title
\author{}
\title{}
\date{September 28, 2013}
\begin{document}

% generates the title
\maketitle

% insert the table of contents
	\tableofcontents

\chapter{Methods}

\section{Improved Memory Use}

One of the greatest optimizations we applied in several algorithms was the better use of the cache memory and the main memory. Some data intensive reading algorithms can benefit from the cache memory by accessing sequential adresses from the main memory, with lesser cache misses, the algorithm has no need to search the data in the main memory, which is much slower than low level memories. \\

By changing the way the memory is accessed, the performance was improved in the sequential version of some algorithms, improving even the performance over some simple parallelized versions.\\

Several algorithms in this benchmark set have nested loops and access some structure, like a array or a matrix, inside the inner loop. Also, some algorithms have inefficient index choice for the inner loop, turning the memory access into a slow and repetitious task. Some changes that improved the performance include the remapping of the structure or/and the better choice of the order for the indexes in the nested loops. \\


\chapter{Optimizations}

\newpage
\section{sgemm}
\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential source code for sgemm,style=base]

	for (int mm = 0; mm < m; ++mm) {
		for (int nn = 0; nn < n; ++nn) {
			float c = 0.0f;
			for (int i = 0; i < k; ++i) {
				float a = A[mm + i * lda]; 
				float b = B[nn + i * ldb];		
				c += a * b;
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel source code for sgemm,style=base]

	@#pragma omp parallel for collapse (2)@
	for (int mm = 0; mm < m; ++mm) {
		for (int nn = 0; nn < n; ++nn) {
			float c = 0.0f;
			for (int i = 0; i < k; ++i) {
				float a = A[mm + i * lda]; 
				float b = B[nn + i * ldb];
				c += a * b;
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}
\end{minipage}
\end{center}

For the matrices multiplication algorithm, it is used an array-represented structure, so, we improved the memory access by transposing the matrices. This way, the index 'i' in the inner loop, is only added instead of multiplied to calculate the correct mapping of the element in the matrix to the array-represented matrix. We needed then to aloccate auxiliary arrays to represent the transposed matrices and then transpose while making the copy from one array to another. \\

To avoid the increase of the execution time of the algorithm, we parallelized this transposition. This can be done because it has no loop-carried dependencies and the data is distributed almost equally for all threads without concurrency, in other words, it is high parallelizable. \\

We created the variables aux1 and aux2 because in the inner loop, these values are constant. The original versions, the sequential and the stock parallel, calculate these constant every iteration of the inner loop, wasting some time in unnecessary multiplications. \\

The access to the array is almost sequential after this changes. Using better the memory, caused some great improvement in execution time of this algoritm. By applying these techniques we could ran our parallel version around 10.7 times faster than sequential version and 3.9 times faster than the stock parallel version.\\

\begin{lstlisting}[caption=Array-represented matrices transposition,style=base]
	tb = (float *) malloc(ldb * n * sizeof(float));
	@# pragma omp parallel for collapse(2)@
	for (i = 0; i < ldb; i++)
		for (j = 0; j < n; j++)
			tb[j * n + i] = B[i * n + j];

	ta = (float *) malloc(lda * m * sizeof(float));
	@# pragma omp parallel for collapse(2)@
	for (i = 0; i < lda; i++)
		for (j = 0; j < m; j++)
			ta[j * m + i] = A[i * m + j];
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel source code for sgemm,style=base]
	@# pragma omp parallel for private(nn, c, i, a, b, mm) collapse(2) schedule (static)@
	for (mm = 0; mm < m; ++mm) {
		for (nn = 0; nn < n; ++nn) {
			c = 0.0f;
			aux1 = mm * lda;
			aux2 = nn * ldb;
			for (i = 0; i < k; ++i) {
				c += ta[i + aux1] * tb[i + aux2];
			}
			C[mm+nn*ldc] = C[mm+nn*ldc] * beta + alpha * c;
		}
	}
\end{lstlisting}

\newpage
\section{stencil}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential source code for stencil,style=base]

	int i, j, k;
	for(i=1;i<nx-1;i++) {
		for(j=1;j<ny-1;j++) {
			for(k=1;k<nz-1;k++) {
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel source code for stencil,style=base]

	int i;
	@#pragma omp parallel for@
	for(i=1;i<nx-1;i++) {
		int j, k;
		for(j=1;j<ny-1;j++) {
			for(k=1;k<nz-1;k++) {
				//#pragma omp critical
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}
\end{minipage}\\
\begin{lstlisting}[caption=Converts from 3D indices to array-represented cube,style=base]	
	#define Index3D(_nx,_ny,_i,_j,_k) ((_i)+_nx*((_j)+_ny*(_k)))
\end{lstlisting}
\end{center}

The source codes above, specially the listing 2.7, show that the access to the cube elements are made by using the inner loop counter to multiply some constant and then map the cube to an array. The sequential access to memory is lost and the performance is impaired by sucessive cache misses. \\

Basically, we exchanged the index from the inner loop with the index of the outer loop. Then we simply parallelized the outer loop, which made our version run around 25.4 times faster than sequential version and around 15.2 times faster than the stock parallel version.\\

\begin{lstlisting}[caption=Our parallel source code for stencil,style=base]
	
	int i, j, k;	
	@#pragma omp parallel for private(j, i)@
	for(k=1;k<nz-1;k++) {
		for(j=1;j<ny-1;j++) {
			for(i=1;i<nx-1;i++) {
				Anext[Index3D (nx, ny, i, j, k)] = 
				(A0[Index3D (nx, ny, i, j, k + 1)] +
				A0[Index3D (nx, ny, i, j, k - 1)] +
				A0[Index3D (nx, ny, i, j + 1, k)] +
				A0[Index3D (nx, ny, i, j - 1, k)] +
				A0[Index3D (nx, ny, i + 1, j, k)] +
				A0[Index3D (nx, ny, i - 1, j, k)])*c1
				- A0[Index3D (nx, ny, i, j, k)]*c0;
			}
		}
	}
\end{lstlisting}

\newpage
\section{mri-gridding}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Auxiliary function source code for sequential mri-gridding,style=base]

		unsigned int k;
		for(k=0; k<size; ++k){
			// compute value to evaluate kernel at
			// v in the range 0:(_width/2)^2
			v = (((float)k)/((float)size))*cutoff2;

			// compute kernel value and store
			(*LUT)[k] = kernel_value_CPU(beta*sqrt(1.0-(v/cutoff2)));
		}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Auxiliary function source code for parallel mri-gridding,style=base]

		unsigned int k;
		@#pragma omp parallel for private(v)@
		for(k=0; k<size; ++k){
			// compute value to evaluate kernel at
			// v in the range 0:(_width/2)^2
			v = (((float)k)/((float)size))*cutoff2;

			// compute kernel value and store
			(*LUT)[k] = kernel_value_CPU(beta*sqrt(1.0-(v/cutoff2)));
		}
\end{lstlisting}
\end{minipage}
\end{center}

\begin{center}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential main loop for mri-gridding,style=base]

	int i;
	for (i=0; i < n; i++){
		ReconstructionSample pt = sample[i];

		float kx = pt.kX;
		float ky = pt.kY;
		float kz = pt.kZ;
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel main loop for mri-gridding,style=base]

	int i;

	@#pragma omp parallel for private(NxL, NxH, NyL, NyH, NzL, NzH, dz2, nz, dx2, nx, dy2, ny, idxZ, idxY, dy2dz2, idx0, v, idx, w)@
	for (i=0; i < n; i++){
		ReconstructionSample pt = sample[i];

		float kx = pt.kX;
		float ky = pt.kY;
		float kz = pt.kZ;
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil sequential data processing code for mri-gridding,style=base]
		/* grid data */
		gridData[idx].real += (w*pt.real);
		gridData[idx].imag += (w*pt.imag);

		/* estimate sample density */
		sampleDensity[idx] += 1.0;
	}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}[caption=Parboil parallel data processing code for mri-gridding,style=base]
		/* grid data */
		@#pragma omp critical (c1)@
		gridData[idx].real += (w*pt.real);
		@#pragma omp critical (c2)@
		gridData[idx].imag += (w*pt.imag);

		/* estimate sample density */
		@#pragma omp critical (c3)@
		sampleDensity[idx] += 1.0;
\end{lstlisting}
\end{minipage}
\end{center}

For this algorithm we basically allocated the lock-free structure and the accumulations are made privately and without concurrency for each thread. When all threads have finished the work, the final result is obtained by accumulating the partial results from each thread. Luckly, there is no loop-carried dependency, and the partial and the final accumulations are high parallelizable.\\

Our parallel version, with the lock-free vector accumulation, have a execution time around 2.5 times faster than the sequential and around 41.6 times faster than the stock parallel version. This happens because of the critical section pragmas, as we can see on listing 2.14. Each thread must execute this regions with a lock to obtain the correct result. This way, all other threads wait to get the lock and enter the critical region. By using this lock with 8 threads, there is a lot of concurrency and racing conditions, making the stock parallel the slowest version we tested, slower even the sequential version.\\

\newpage
\begin{center}
\begin{lstlisting}[caption=Our parallel lock-free structure allocation for mri-gridding,style=base]

	@#pragma omp parallel private(NxL, NxH, NyL, NyH, NzL, NzH, dz2, nz, dx2,         \
		nx, dy2, ny, idxZ, idxY, dy2dz2, idx0, v, idx, w, t_id)@
	{
		@#pragma omp single@
		{
			if (big_table == NULL)
			{
				//			  printf("Alocou!\n");
				size = params.gridSize[0]*params.gridSize[1]*params.gridSize[2];
				numthreads = omp_get_max_threads();	  
				big_table = (cmplx **) malloc(numthreads * sizeof(cmplx *));
				big_table2 = (float **) malloc(numthreads * sizeof(float *));
			}
		}

		@#pragma omp for@
		for (i = 0; i < numthreads; i++)
		{
			big_table[i] = (cmplx *) malloc(size * sizeof(cmplx));
			big_table2[i] = (float *) malloc(size * sizeof(float));
		}
		@#pragma omp for collapse(2)@
		for (i = 0; i < numthreads; i++)
			for (j = 0; j < size; j++)
			{
				big_table[i][j].real = 0;
				big_table[i][j].imag = 0;
				big_table2[i][j] = 0;
			}

		t_id = omp_get_thread_num();

		@#pragma omp for@
		for (i = 0; i < n; i++)
		{
			ReconstructionSample pt = sample[i];

			float kx = pt.kX;
			float ky = pt.kY;
			float kz = pt.kZ;
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel partial data processing code for mri-gridding,style=base]
		/* grid data */
		big_table[t_id][idx].real += (w*pt.real);
		big_table[t_id][idx].imag += (w*pt.imag);

		/* estimate sample density */
		big_table2[t_id][idx] += 1.0;		
\end{lstlisting}

\begin{lstlisting}[caption=Our parallel final data processing code for mri-gridding,style=base]
	#pragma omp for private(j)
	for (i = 0; i < size; i++)
		for (j = 0; j < numthreads; j++)
		{
			gridData[i].real += big_table[j][i].real;
			gridData[i].imag += big_table[j][i].imag;
			sampleDensity[i] += big_table2[j][i];
		}
\end{lstlisting}
\end{center}

\end{document}
